1. NEF (Network Exposure Function):
In this project, I worked on NEF, which allows certain internal network functions to be safely shared with the outside world, 
while keeping sensitive data secure.

What I did:
I helped with database operations and caching to make the app faster and more reliable.

The main part I built myself was the NP Service API:
I created APIs that send HTTP requests to different servers to get, update, or delete network data.
I saved the responses in a Redis database so they could be used again quickly later.
This helped in exposing only the needed network details to the outside, while keeping everything secure.
 

2. BR (Backup and Restore):
This is a tool that helps take manual or scheduled backups of a network node (its data and settings), and restore them if 
needed.
It's very useful if a node crashes or stops working properly.

What I did:
I worked on both the frontend and backend of this project from the beginning.
On the frontend (Angular) side, I built most of the UI to make it simple and easy to use.
On the backend, I made APIs to handle things like:
Taking backups
Restoring data
Showing backup reports
Sending pop-up notifications to show status
We also added features like profiling and detailed status reporting to make it more helpful for users.
 
 In short:
I‚Äôve worked on both backend and frontend. I take full ownership of the tasks I‚Äôm given, and I focus on building apps that are 
reliable, fast, and easy to use.


ensured redundancy - HSM
cut deployment time by 30% - autoinstall script

explain project
how you implement rest api
node - mobiles directly connected to nodes. nodes present in tower.


üîπ Interview Explanation

Here‚Äôs how you can explain it in an interview:

‚ÄúI implemented REST APIs using Jetty embedded server with Servlets.
In my setup, I used HttpServlet classes where I override methods like doGet, doPost, doPut, and doDelete to implement CRUD operations.
I used Jackson for JSON serialization/deserialization.
For server configuration, Jetty allows two approaches:

Raw Jetty API where you directly create a Server, configure ServletContextHandler, and attach servlets.

In my project, we used a custom wrapper called JettyBuilder that simplifies this by providing methods like .setPort(), .addHandler(), and .start(). Internally it uses Jetty APIs but makes configuration cleaner.
This way, I could map REST endpoints like /api/users to my UserServlet and handle client requests with proper JSON responses.‚Äù

===========================================================================
üåê REST API with Servlets & Jetty (Full Demo + Explanation)
1. What is a Servlet?
A Servlet is a Java class that handles HTTP requests and generates responses.
It‚Äôs part of Java EE, but we can run it in embedded servers like Jetty or Tomcat.

Lifecycle:
init() ‚Üí runs once when servlet loads.
service() ‚Üí called for every request (delegates to doGet, doPost, etc.).
destroy() ‚Üí runs once when shutting down.

2. What is Jetty?
Jetty is a lightweight Java-based web server & servlet container.
You can:
Deploy WAR files into a Jetty installation (traditional way).
OR run embedded Jetty inside your Java app (modern way, no web.xml needed).

3. What is a REST API?
REST (Representational State Transfer) is an architectural style for APIs.
It uses HTTP methods to operate on resources:
GET ‚Üí Read data
POST ‚Üí Create data
PUT ‚Üí Update data
DELETE ‚Üí Delete data
Data is usually exchanged in JSON format.

4. Demo 1: REST API with Jetty (without Jetty Builder)
import org.eclipse.jetty.server.Server;
import org.eclipse.jetty.servlet.ServletContextHandler;
import org.eclipse.jetty.servlet.ServletHolder;

import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import java.io.IOException;

// Step 1: Create a REST Servlet
public class HelloServlet extends HttpServlet {
    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws IOException {
        resp.setContentType("application/json");
        resp.getWriter().write("{\"message\": \"Hello, this is GET response\"}");
    }

    @Override
    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws IOException {
        resp.setContentType("application/json");
        resp.getWriter().write("{\"message\": \"Hello, this is POST response\"}");
    }
}

// Step 2: Setup Embedded Jetty
public class JettyApp {
    public static void main(String[] args) throws Exception {
        // Jetty Server runs on port 8080
        Server server = new Server(8080);

        // Map servlet to /api/*
        ServletContextHandler handler = new ServletContextHandler(ServletContextHandler.SESSIONS);
        handler.setContextPath("/");
        server.setHandler(handler);

        handler.addServlet(new ServletHolder(new HelloServlet()), "/api/hello");

        // Start server
        server.start();
        System.out.println("Server started at http://localhost:8080/api/hello");
        server.join();
    }
}


üëâ Run JettyApp ‚Üí open in browser:

http://localhost:8080/api/hello ‚Üí returns JSON

Use Postman ‚Üí send POST request ‚Üí different response.

5. Demo 2: REST API with Jetty Builder (Fluent API)
import org.eclipse.jetty.server.Server;
import org.eclipse.jetty.servlet.ServletHandler;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import java.io.IOException;

public class JettyBuilderDemo {
    public static void main(String[] args) throws Exception {
        // Fluent-style server builder
        Server server = new Server(9090);

        ServletHandler handler = new ServletHandler();
        server.setHandler(handler);

        handler.addServletWithMapping(HelloServlet2.class, "/api/hi");

        server.start();
        System.out.println("Server started at http://localhost:9090/api/hi");
        server.join();
    }

    // REST Servlet
    public static class HelloServlet2 extends HttpServlet {
        @Override
        protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws IOException {
            resp.setContentType("application/json");
            resp.getWriter().write("{\"message\": \"Hello from JettyBuilder!\"}");
        }
    }
}

6. How to Explain in Interview üöÄ

When asked ‚ÄúHow did you implement REST API?‚Äù ‚Üí Answer like this:

I implemented REST APIs using Servlets on an embedded Jetty server.
First, I created Servlets that override doGet, doPost, etc., to handle REST operations and return JSON responses.
Then, I configured Jetty in Java code itself ‚Äî instead of web.xml ‚Äî by creating a Server, attaching ServletContextHandler, 
and mapping servlets to specific endpoints like /api/hello.
This gave me full control without needing an external container.
For testing, I used Postman and verified GET/POST/DELETE requests.

======================================================================
üîπ 1. Raw Jetty API (without JettyBuilder)
Step 1: User Model
public class User {
    private int id;
    private String name;
    private String email;

    public User() {}
    public User(int id, String name, String email) {
        this.id = id; this.name = name; this.email = email;
    }

    public int getId() { return id; }
    public void setId(int id) { this.id = id; }
    public String getName() { return name; }
    public void setName(String name) { this.name = name; }
    public String getEmail() { return email; }
    public void setEmail(String email) { this.email = email; }
}

Step 2: UserServlet (CRUD)
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.servlet.http.HttpServlet;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class UserServlet extends HttpServlet {
    private final List<User> users = new ArrayList<>();
    private final ObjectMapper mapper = new ObjectMapper();

    @Override
    public void init() {
        users.add(new User(1, "Sonali", "sonali@example.com"));
        users.add(new User(2, "Rahul", "rahul@example.com"));
    }

    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws IOException {
        resp.setContentType("application/json");
        mapper.writeValue(resp.getOutputStream(), users);
    }

    @Override
    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws IOException {
        User newUser = mapper.readValue(req.getInputStream(), User.class);
        newUser.setId(users.size() + 1);
        users.add(newUser);
        resp.setContentType("application/json");
        mapper.writeValue(resp.getOutputStream(), newUser);
    }

    @Override
    protected void doPut(HttpServletRequest req, HttpServletResponse resp) throws IOException {
        String path = req.getPathInfo(); // /1
        int id = Integer.parseInt(path.substring(1));
        User updated = mapper.readValue(req.getInputStream(), User.class);
        for (User u : users) {
            if (u.getId() == id) {
                u.setName(updated.getName());
                u.setEmail(updated.getEmail());
            }
        }
        resp.setContentType("application/json");
        mapper.writeValue(resp.getOutputStream(), updated);
    }

    @Override
    protected void doDelete(HttpServletRequest req, HttpServletResponse resp) {
        String path = req.getPathInfo(); // /1
        int id = Integer.parseInt(path.substring(1));
        users.removeIf(u -> u.getId() == id);
        resp.setStatus(HttpServletResponse.SC_NO_CONTENT);
    }
}

Step 3: Start Jetty (Raw)
import org.eclipse.jetty.server.Server;
import org.eclipse.jetty.servlet.ServletContextHandler;

public class JettyRawCRUD {
    public static void main(String[] args) throws Exception {
        Server server = new Server(8080);

        ServletContextHandler handler = new ServletContextHandler(ServletContextHandler.SESSIONS);
        handler.setContextPath("/");
        handler.addServlet(UserServlet.class, "/api/users/*");

        server.setHandler(handler);
        server.start();
        System.out.println("Raw Jetty CRUD running at http://localhost:8080/api/users");
        server.join();
    }
}

üîπ 2. JettyBuilder API (Wrapper style)
Step 1: JettyBuilder (simplified wrapper)
import org.eclipse.jetty.server.Server;
import org.eclipse.jetty.servlet.ServletContextHandler;

public class JettyBuilder {
    private Server server;
    private ServletContextHandler handler;

    public JettyBuilder setPort(int port) {
        server = new Server(port);
        handler = new ServletContextHandler(ServletContextHandler.SESSIONS);
        handler.setContextPath("/");
        return this;
    }

    public JettyBuilder addHandler(String path, Class<? extends jakarta.servlet.http.HttpServlet> servletClass) {
        handler.addServlet(servletClass, path);
        return this;
    }

    public void start() throws Exception {
        server.setHandler(handler);
        server.start();
        System.out.println("JettyBuilder CRUD server running...");
        server.join();
    }
}

Step 2: Start Jetty with JettyBuilder
public class JettyBuilderCRUD {
    public static void main(String[] args) throws Exception {
        new JettyBuilder()
            .setPort(9090)
            .addHandler("/api/users/*", UserServlet.class)
            .start();
    }
}

üîπ 3. Test the API
Method	URL	Description
GET	http://localhost:8080/api/users	Get all users
POST	http://localhost:8080/api/users	Create user (body JSON: {"name":"Amit","email":"amit@example.com"})
PUT	http://localhost:8080/api/users/1	Update user with id=1
DELETE	http://localhost:8080/api/users/2	Delete user with id=2

Example using curl:

curl -X GET http://localhost:8080/api/users
curl -X POST http://localhost:8080/api/users -H "Content-Type: application/json" -d '{"name":"Amit","email":"amit@example.com"}'
curl -X PUT http://localhost:8080/api/users/1 -H "Content-Type: application/json" -d '{"name":"Sonali Updated","email":"sonali@new.com"}'
curl -X DELETE http://localhost:8080/api/users/2

üîπ 4. How to Explain in Interview

‚ÄúI implemented a full CRUD REST API using Java Servlets and embedded Jetty.
Each endpoint (GET, POST, PUT, DELETE) is handled by a UserServlet.
I use Jackson to serialize/deserialize JSON.
For server setup, I have two approaches:

Raw Jetty: directly creating Server, ServletContextHandler, and mapping servlets.

JettyBuilder: a custom wrapper in our project that simplifies server setup with .setPort(), .addHandler(), and .start().
Internally, JettyBuilder uses the same Jetty APIs, but it reduces boilerplate.
Data is stored in-memory using List<User> for demonstration, but in production it could be connected to a database.‚Äù

Perfect ‚úÖ Let‚Äôs create a visual flow diagram of a REST API request using Servlet + Jetty. This is exactly how you can explain
 it in an interview.

üåê Flow Diagram: REST API with Jetty + Servlet
   CLIENT (Browser / Postman / App)
             |
             |  HTTP Request (GET/POST/PUT/DELETE)
             v
   --------------------------
   |     Embedded Jetty     |
   |------------------------|
   |  Server(port 8080)     |
   |  ServletContextHandler  |
   --------------------------
             |
             |  Dispatch request to matching servlet
             v
   --------------------------
   |      UserServlet       |
   |------------------------|
   |  doGet() / doPost()    |
   |  doPut() / doDelete()  |
   --------------------------
             |
             |  Perform business logic
             |  (CRUD operations in memory)
             v
   --------------------------
   |      Response JSON     |
   |  {"id":1,"name":"..."} |
   --------------------------
             |
             v
         CLIENT receives JSON

üîπ Explanation of the Flow (Interview style)

Client: Could be a browser, Postman, or mobile app sending HTTP requests (GET, POST, etc.).
Jetty Server: Embedded Jetty listens on a port (e.g., 8080) and receives the request.
ServletContextHandler: Jetty‚Äôs handler maps the URL path (/api/users/*) to the corresponding Servlet.
UserServlet: The Servlet handles the request:
doGet() ‚Üí fetch data
doPost() ‚Üí create new data
doPut() ‚Üí update data
doDelete() ‚Üí delete data
It uses Jackson to convert Java objects to JSON and vice versa.
Business Logic: In our demo, it‚Äôs simple in-memory CRUD (List<User>).
Response: Servlet writes a JSON response back to Jetty, which sends it to the client.

üîπ Key Points to Mention in Interview
No web.xml: Jetty allows configuring everything programmatically.
Embedded Server: Easy to start/stop server inside Java app.
Servlet Lifecycle: init(), service(), destroy() ‚Üí explain if asked.
JSON Serialization: Using Jackson to convert Java objects to JSON.
Flexibility: You can use either:
Raw Jetty API ‚Üí full control
JettyBuilder ‚Üí cleaner code, less boilerplate

Perfect ‚úÖ Let‚Äôs make a side-by-side visual diagram showing Raw Jetty vs JettyBuilder. This is very effective for interviews
 because it clearly shows what happens under the hood and how the wrapper simplifies setup.

üåê Side-by-Side Flow: Raw Jetty vs JettyBuilder
          CLIENT
      (Browser / Postman)
              |
              | HTTP Request (GET/POST/PUT/DELETE)
              v
     -----------------------------------
     |          Raw Jetty              |
     |---------------------------------|
     |  Server(port 8080)              |
     |  ServletContextHandler           |
     |  addServlet(UserServlet, /api)  |
     -----------------------------------
              |
              | Dispatch request
              v
        ----------------
        | UserServlet  |
        | doGet/doPost |
        ----------------
              |
              | Business Logic (CRUD)
              v
        ----------------
        | JSON Response |
        ----------------
              |
              v
           CLIENT receives JSON

          CLIENT
      (Browser / Postman)
              |
              | HTTP Request (GET/POST/PUT/DELETE)
              v
     -----------------------------------
     |       JettyBuilder Wrapper       |
     |---------------------------------|
     |  JettyBuilder.setPort(8080)     |
     |  JettyBuilder.addHandler(...)   |
     |  JettyBuilder.start()           |
     -----------------------------------
              |
              | Internally:
              | Creates Server + ServletContextHandler
              | Maps servlet to path
              v
        ----------------
        | UserServlet  |
        | doGet/doPost |
        ----------------
              |
              | Business Logic (CRUD)
              v
        ----------------
        | JSON Response |
        ----------------
              |
              v
           CLIENT receives JSON

üîπ How to Explain in Interview

‚ÄúOn the left, you see Raw Jetty: I manually create a Server, attach a ServletContextHandler, and map my UserServlet to
 /api/users/*. Jetty handles incoming HTTP requests, dispatches them to the servlet, which performs CRUD operations and returns
 JSON.
On the right, we have JettyBuilder, a custom wrapper in our project. Instead of writing all boilerplate, I call .setPort(), 
.addHandler(), and .start(). Internally, it does the same: creates a Jetty server, attaches handlers, and dispatches requests 
to the servlet. This simplifies setup and reduces repetitive code.‚Äù

==================================================================================

üåê 1. Detailed Flow: CRUD REST API via JettyBuilder
         CLIENT
  (Browser / Postman / App)
            |
            | HTTP Request
            | (GET / POST / PUT / DELETE)
            v
  ----------------------------------------
  |          JettyBuilder Wrapper        |
  |--------------------------------------|
  |  1. setPort(8080)                     |
  |  2. addHandler("/api/users/*",       |
  |      UserServlet.class)               |
  |  3. start()                           |
  ----------------------------------------
            |
            | Internally creates:
            |  - Jetty Server
            |  - ServletContextHandler
            |  - Servlet mapping
            v
        ----------------
        | UserServlet  |
        | doGet/doPost |
        | doPut/doDel  |
        ----------------
            |
            | Business Logic:
            | - GET ‚Üí fetch users
            | - POST ‚Üí create new user
            | - PUT ‚Üí update user
            | - DELETE ‚Üí remove user
            v
        ----------------
        | JSON Response |
        ----------------
            |
            v
         CLIENT receives JSON


Example Mapping in JettyBuilder:

new JettyBuilder()
    .setPort(8080)
    .addHandler("/api/users/*", UserServlet.class)
    .start();


JettyBuilder internally does:
Server server = new Server(port);
ServletContextHandler handler = new ServletContextHandler(...);
handler.addServlet(UserServlet.class, "/api/users/*");
server.setHandler(handler); server.start();

üåê 2. Why Jetty? (Deep Explanation)
Jetty is a lightweight, embeddable Java HTTP server and servlet container.

Key Points:
Embeddable
Jetty can run inside your Java application, no need for external WAR deployment.
Perfect for microservices or internal services.
Lightweight & Fast
Minimal footprint.
Very fast startup, low memory usage.
Asynchronous Support
Jetty supports non-blocking requests (async servlet API).
Useful for high-concurrency environments.
Highly Configurable
Thread pool, connectors, handlers, HTTPS, WebSocket support.
Production Ready
Powers big systems like Eclipse IDE, Jenkins, 5G network interfaces (like your NEF).

üåê 3. Jetty vs Other Servers
Feature			Jetty			Tomcat					Undertow
Embeddable		‚úÖ				‚úÖ (slightly heavier)	‚úÖ
Memory Usage	Low				Medium					Very Low
Async Support	‚úÖ Excellent		Limited					‚úÖ Excellent
Lightweight		‚úÖ				Moderate				‚úÖ
Startup Time	Very Fast		Moderate				Very Fast
WebSocket		‚úÖ				‚úÖ						‚úÖ
Use Case		Microservices, embedded servers, IoT, telecom	Enterprise apps, traditional webapps	Microservices, high-concurrency
Community / Production Proven	‚úÖ	‚úÖ	‚úÖ

Why we use Jetty in your project:
Embedded ‚Üí No external app server, easy CI/CD.
Lightweight & Fast ‚Üí Handles high-throughput 5G requests efficiently.
Async support ‚Üí Required for high concurrency (like NEF).
Customizable via JettyBuilder ‚Üí Developers don‚Äôt have to write boilerplate.
Stable & widely used in telecom and microservices ‚Üí Proven for production.

üåê 4. How to Explain in Interview

‚ÄúI implemented REST APIs using Servlets and embedded Jetty. Jetty acts as an HTTP server that receives requests, dispatches them to Servlets, and sends responses.
I used JettyBuilder, a wrapper in our project, to configure server port, servlet paths, and start the server programmatically. Internally, JettyBuilder uses the Server + ServletContextHandler from Jetty.
Jetty is chosen over other servers like Tomcat because it is lightweight, embeddable, fast, and supports asynchronous request handling, which is critical for high-concurrency scenarios like our 5G network services.
Each HTTP method (GET/POST/PUT/DELETE) is mapped to a method in the Servlet (doGet/doPost/doPut/doDelete) that performs business logic and returns JSON responses.‚Äù


Perfect ‚úÖ Let‚Äôs create a visual diagram showing GET/POST/PUT/DELETE separately for a Jetty + Servlet REST API, so you can explain each HTTP method clearly in an interview.

üåê CRUD Flow: Jetty + Servlet (Separate for Each HTTP Method)
                     CLIENT
              (Browser / Postman / App)
                             |
        ----------------------------------------------------
        |                  HTTP METHOD                     |
        |--------------------------------------------------|
        | GET     | POST        | PUT             | DELETE |
        ----------------------------------------------------
                             |
                             v
                    Embedded Jetty Server
          (Server + ServletContextHandler)
                             |
         Dispatches request to mapped servlet
                             v
                      UserServlet
         ------------------------------------------------
         | GET    ‚Üí doGet()      ‚Üí return List<User>   |
         | POST   ‚Üí doPost()     ‚Üí create User & return |
         | PUT    ‚Üí doPut()      ‚Üí update User & return |
         | DELETE ‚Üí doDelete()   ‚Üí remove User          |
         ------------------------------------------------
                             |
                             v
                        JSON Response
         ------------------------------------------------
         | GET    ‚Üí [{"id":1,"name":"Sonali",...}, ...] |
         | POST   ‚Üí {"id":3,"name":"Amit","email":"..."}|
         | PUT    ‚Üí {"id":1,"name":"Sonali Updated"}   |
         | DELETE ‚Üí 204 No Content                      |
         ------------------------------------------------
                             |
                             v
                        CLIENT RECEIVES DATA

üîπ Interview Explanation for Each Method

GET:
Client requests /api/users.
Jetty receives the request, forwards to doGet().
Servlet fetches users from memory and sends JSON back.

POST:
Client sends new user data in JSON body.
Jetty forwards request to doPost().
Servlet parses JSON (Jackson), creates user, adds to list, returns created user JSON.

PUT:
Client sends updated user data to /api/users/{id}.
Jetty routes request to doPut().
Servlet finds user by ID, updates info, returns updated JSON.

DELETE:
Client sends request to /api/users/{id}.
Jetty forwards to doDelete().
Servlet removes the user, returns HTTP 204 No Content.

üîπ Extra Jetty Points to Mention
Jetty maps paths to servlets using ServletContextHandler.
Jetty handles concurrency with a thread pool; multiple clients can call APIs at the same time.
Jetty supports async processing, which is useful for high-volume services like your NEF project.
Using JettyBuilder makes it easier to configure multiple handlers, ports, and threads without writing repetitive code.

===============================================================================

Perfect ‚úÖ Let‚Äôs make a visual diagram for your NEF JettyBuilder setup showing multiple handlers (PING, ALARM, CONFIG, CLI, KPI, etc.), so you can explain your real project architecture to the interviewer clearly.

üåê NEF Project: JettyBuilder Multiple Handlers Flow
                 CLIENT / SYSTEM
           (External Requests, APIs, CLI)
                          |
                          v
                 Embedded Jetty Server
        (JettyBuilder + Server + ServletContextHandler)
                          |
          -----------------------------------------
          |           Handlers / Servlets        |
          -----------------------------------------
          | 1. /ping        ‚Üí RTJioNEFPingHandler        |
          | 2. /alarm       ‚Üí RTJioNEFAlarmHandler       |
          | 3. /config      ‚Üí RTJioNEFConfigHandler      |
          | 4. /reping      ‚Üí RTJioNEFRePingHandler     |
          | 5. /counter     ‚Üí RtJioNEFCounterHandler    |
          | 6. /cli         ‚Üí RTJioNEFCLIHandler        |
          | 7. /kpi         ‚Üí RTJioNEFCounterKpiHandler |
          | 8. /nrf         ‚Üí RTJioNrfCommunicationHandler |
          | 9. /capif       ‚Üí RTJioCapifPublishHandler  |
          -----------------------------------------
                          |
         Each Handler performs its **business logic**:
         - PING ‚Üí health check
         - ALARM ‚Üí raise or clear alarms
         - CONFIG ‚Üí read/update configuration
         - REPING ‚Üí reconnect or recheck status
         - COUNTER ‚Üí metrics & counters
         - CLI ‚Üí command-line interface execution
         - KPI ‚Üí key performance indicator calculation
         - NRF ‚Üí communication with NRF node
         - CAPIF ‚Üí publish data to CAPIF system
                          |
                          v
                 JSON / Response Output
                          |
                          v
                  CLIENT / SYSTEM receives data

üîπ How to Explain in Interview (NEF Context)

‚ÄúIn our NEF project, we used JettyBuilder to configure an embedded Jetty server.
We have multiple handlers, each mapped to a specific URL context like /ping, /alarm, /config, /cli, etc.
Jetty receives incoming HTTP requests, checks the URL, and dispatches it to the corresponding handler.
Each handler contains its business logic: for example, /ping performs a health check, /alarm raises or clears alarms, /cli executes commands, and /kpi calculates key performance metrics.
Responses are sent back in JSON format.
Using JettyBuilder allowed us to programmatically configure multiple handlers, control threads, set IP/ports, and avoid writing repetitive server setup code.
Jetty is lightweight, fast, and supports asynchronous operations, which is ideal for a high-performance, telecom-grade environment like 5G NEF.‚Äù

üîπ Extra Jetty + NEF Talking Points

JettyBuilder Advantages:
Simplifies adding multiple handlers (addHandler("/ping", RTJioNEFPingHandler.class))
Supports multiple IPs and ports
Configurable thread pool for high concurrency
Concurrency:
Each handler can serve multiple requests simultaneously
Jetty internally uses thread pools for efficient request processing
JSON Responses:
Each handler sends structured JSON back to the client/system
Makes it easy for monitoring tools or API clients to parse data
Production Ready:
Jetty is proven for telecom services
Supports embedded deployment ‚Üí no external server dependency
Lightweight ‚Üí low memory and CPU footprint

=============================================================================

Perfect ‚úÖ Let‚Äôs go deep into Jetty internals and explain how a request flows through Jetty to your handler and returns a response, including thread pools, so you can impress the interviewer with your knowledge.

üåê Jetty Internal Flow: Request ‚Üí Handler ‚Üí Response
            CLIENT / SYSTEM
      (Browser, Postman, NEF System)
                      |
                      | HTTP Request
                      v
            Embedded Jetty Server
      (Server + Connector + Thread Pool)
                      |
        ------------------------------------------------
        | Jetty Connector (Accepts TCP Connections)  |
        ------------------------------------------------
                      |
          Jetty Thread Pool assigns a thread
                      |
        ------------------------------------------------
        | HTTP Request Dispatcher / Handler          |
        |------------------------------------------|
        | ServletContextHandler / JettyHandler      |
        ------------------------------------------------
                      |
          Path Mapping: Checks URL context
   (e.g., /ping ‚Üí RTJioNEFPingHandler)
                      |
                      v
                Handler (Servlet)
        ---------------------------------
        | doGet/doPost/doPut/doDelete |
        | Business Logic Execution     |
        ---------------------------------
                      |
        Generates Response (JSON, Status Code)
                      |
        Jetty sends Response back through Connector
                      |
                      v
            CLIENT receives HTTP Response

üîπ Step-by-Step Explanation (Interview Style)

Connector Accepts Request:
Jetty has a Connector (e.g., ServerConnector) that listens on the configured IP and port.
Accepts TCP connections from clients.

Thread Pool Assignment:
Jetty maintains a thread pool.
Each incoming request is assigned to a thread from the pool.
This ensures high concurrency and efficient request handling.

Dispatcher / Handler Mapping:
Jetty checks the request path (/ping, /alarm, /config, etc.)
Dispatches the request to the appropriate handler or servlet.

Handler Execution:
The mapped handler (e.g., RTJioNEFPingHandler) executes the business logic.
For REST APIs, it reads the request, interacts with data, and prepares a JSON response.

Response Handling:
Handler writes the response to HttpServletResponse
Jetty sends it back to the client through the same TCP connection.

Thread Reuse:
Once the request is processed, the thread returns to the pool.
Efficiently serves new incoming requests.

üîπ Why This is Strong for Interview
You can explain Jetty is not just a web server, it‚Äôs a full servlet container with its own threading model.
You can highlight why embedded Jetty + JettyBuilder is perfect for NEF:
Handles multiple concurrent requests efficiently.
Allows custom handlers for each service (/ping, /alarm, /cli, /kpi).
Lightweight and production-ready for telecom-grade applications.
Thread pool + async support ensures low latency and high throughput.

üîπ Example Talking Points for NEF

‚ÄúIn our NEF project, Jetty accepts incoming requests through a connector, assigns a thread from its pool, and dispatches the
 request to the appropriate handler like /ping or /alarm.
The handler executes business logic and writes JSON back to HttpServletResponse.
Jetty efficiently reuses threads for new requests, supporting high concurrency in 5G services.
Using JettyBuilder, we configure multiple handlers, ports, and IPs programmatically, avoiding repetitive boilerplate and making
 our system highly maintainable.‚Äù
 
==============================================================================

üåê Interview Explanation Script: NEF REST API with JettyBuilder

1. Introduction / Setup
‚ÄúIn our NEF project, we implemented REST APIs using Servlets and embedded Jetty.
We chose Jetty because it is lightweight, fast, embeddable, and supports asynchronous processing, which is essential for high-throughput 5G services.
Instead of deploying to an external server, we used JettyBuilder, a wrapper that simplifies server configuration and handler management.‚Äù

2. JettyBuilder Overview
‚ÄúUsing JettyBuilder, we programmatically set:
The server port and IP addresses
Thread pool configuration for handling multiple concurrent requests
Multiple handlers for different URL paths, like /ping, /alarm, /config, /cli, /kpi, /nrf, and /capif.
This avoids boilerplate code and makes the server highly maintainable.‚Äù

3. Request Flow (Master Diagram Walkthrough)
‚ÄúHere‚Äôs how a request flows:
The client sends an HTTP request (GET, POST, PUT, DELETE).
Jetty‚Äôs Connector accepts the TCP connection and assigns a thread from the pool.
Jetty checks the URL path and dispatches the request to the appropriate handler/servlet.
Each handler executes its business logic:
/ping ‚Üí health check
/alarm ‚Üí raise/clear alarms
/config ‚Üí read/update configuration
/cli ‚Üí execute commands
/kpi ‚Üí calculate key performance metrics
/nrf ‚Üí communicate with NRF node
/capif ‚Üí publish to CAPIF system
The handler writes a JSON response.
Jetty sends the response back to the client.
The thread returns to the pool for reuse.‚Äù

4. Thread Pool & Concurrency
‚ÄúJetty‚Äôs thread pool allows multiple requests to be processed simultaneously.
This is crucial for NEF services in a 5G network, where multiple clients or network nodes may query metrics, alarms, or configuration at the same time.
Using JettyBuilder, we can also configure min/max threads, ensuring optimal resource usage.‚Äù

5. Advantages of Jetty in Our Project
‚ÄúWe use Jetty over other servers like Tomcat or Undertow because:
It is lightweight and embeddable, perfect for microservices.
It supports asynchronous requests, which improves performance under high concurrency.
JettyBuilder makes adding multiple handlers and configuring threads easy.
It has a proven track record in telecom and high-performance systems, which aligns with our 5G NEF requirements.‚Äù
6. Closing / Summary
‚ÄúSo in summary, our NEF REST API system uses embedded Jetty with JettyBuilder to handle multiple services efficiently.
Requests are dispatched to the correct handler, processed concurrently using threads, and responses are sent in JSON format.
This setup ensures a highly performant, maintainable, and production-ready REST API framework.‚Äù

==============================================================================

üåê FULL INTERVIEW-READY NEF REST API EXPLANATION
1Ô∏è‚É£ Elevator Version (5-Minutes)

‚ÄúIn our NEF project, I implemented REST APIs using embedded Jetty with Servlets. Jetty is lightweight, fast, embeddable, and supports asynchronous requests, which is essential for high-throughput 5G services.
We used JettyBuilder, a wrapper that allows us to configure server ports, IPs, thread pools, and multiple handlers programmatically without web.xml.
Handlers are mapped to paths like /ping, /alarm, /config, /cli, /kpi, /nrf, and /capif.
When a request comes in: Jetty assigns a thread from its pool, dispatches the request to the correct handler, executes business logic, and returns a JSON response.
Each handler has a specific role: /ping for health checks, /alarm for raising or clearing alarms, /cli for command execution, and /kpi for performance metrics.
Using JettyBuilder ensures high concurrency, maintainability, and production-ready performance for telecom-grade 5G NEF services.‚Äù

2Ô∏è‚É£ Detailed NEF Setup with JettyBuilder

Master Flow Diagram:
CLIENT ‚Üí Embedded Jetty (JettyBuilder) ‚Üí Connector ‚Üí Thread Pool ‚Üí Handler ‚Üí JSON Response ‚Üí CLIENT


Handlers:
/ping ‚Üí RTJioNEFPingHandler
/alarm ‚Üí RTJioNEFAlarmHandler
/config ‚Üí RTJioNEFConfigHandler
/reping ‚Üí RTJioNEFRePingHandler
/counter ‚Üí RtJioNEFCounterHandler
/cli ‚Üí RTJioNEFCLIHandler
/kpi ‚Üí RTJioNEFCounterKpiHandler
/nrf ‚Üí RTJioNrfCommunicationHandler
/capif ‚Üí RTJioCapifPublishHandler

Explanation:
JettyBuilder simplifies server setup, adds handlers, sets ports/IPs, and configures thread pools.
Each handler executes specific business logic.
Responses are always JSON for consistency and easy client parsing.

3Ô∏è‚É£ Jetty Internal Flow & Concurrency
HTTP Request ‚Üí Connector ‚Üí Thread Pool ‚Üí ServletContextHandler ‚Üí Specific Handler ‚Üí Response ‚Üí Client


Step-by-Step:
Connector accepts TCP connection.
Thread pool assigns a thread.
Jetty maps request URL to handler.
Handler executes logic (CRUD, metrics, alarms, CLI).
Handler writes JSON response.
Jetty sends response back to client.
Thread returns to pool for reuse.

Why this matters:
High concurrency: multiple requests processed simultaneously.
Low latency: threads are efficiently reused.
Asynchronous support: scalable for 5G network demands.

4Ô∏è‚É£ Why Jetty over Other Servers
Feature				Jetty					Tomcat				Undertow
Embeddable			‚úÖ						‚úÖ					‚úÖ
Lightweight			‚úÖ						Moderate			‚úÖ
Async Support		‚úÖ Excellent				Limited				‚úÖ Excellent
Thread Pool Config	‚úÖ						Moderate			‚úÖ
Use Case			Microservices, telecom	Enterprise web apps	Microservices, high-concurrency
Startup Time		Fast					Moderate			Very Fast
Production Proven	‚úÖ						‚úÖ					‚úÖ

Why Jetty in NEF:
Lightweight & embeddable ‚Üí runs inside the application.
High concurrency ‚Üí thread pool efficiently handles multiple requests.
Asynchronous support ‚Üí critical for telecom-grade 5G services.
JettyBuilder reduces boilerplate, simplifies handler management.

5Ô∏è‚É£ Sample Follow-Up Questions & Strong Answers

Q1: Why not Tomcat or other servers?
Jetty is more lightweight, embeddable, and supports async processing better. Perfect for high-throughput microservices and telecom-grade systems like NEF.

Q2: How does Jetty handle concurrency?
Jetty uses a thread pool. Each incoming request gets a thread, executes the mapped handler, and returns to the pool. Multiple requests are processed simultaneously without blocking.

Q3: How does JettyBuilder work internally?
JettyBuilder is a wrapper: internally it creates a Server, adds ServletContextHandlers, maps handlers to URL paths, configures thread pools, and starts the server programmatically.

Q4: How are JSON responses generated?
Each handler uses HttpServletResponse to write JSON, usually via libraries like Jackson, for consistent client parsing.

Q5: How is this architecture production-ready?
Embedded Jetty + JettyBuilder ensures low memory usage, high concurrency, configurable threads, and modular handlers, making it stable and maintainable for 24/7 telecom services.

=======================================================================
in jetty - Lightweight & embeddable ‚Üí runs inside the application.
 means in tomcat do we need different server

Exactly! Let me clarify this carefully:
Jetty (Lightweight & Embeddable)
Jetty can be embedded directly inside your Java application.
This means you don‚Äôt need to install or start a separate server; your app itself starts Jetty when it runs.
Useful for microservices, REST APIs, or quick demos.
Example: Your main() method can start Jetty, deploy your servlet, and your app is ready.

Tomcat (Traditional)
Usually, Tomcat is a standalone server.
You deploy your .war file to Tomcat, which runs independently of your application.
You need a separate server process for Tomcat to serve your app.
There is an embedded version of Tomcat (like Spring Boot uses), but traditionally, it‚Äôs a separate server.

‚úÖ So the key difference:
Jetty: often embedded ‚Üí no separate server needed.
Tomcat (traditional): separate server ‚Üí deploy app there.

======================================================================

why redis - 
"We used Redis in NEF because we didn‚Äôt need the data to persist for a long time. Since Redis is an in-memory database, it 
provides extremely fast read and write operations, ideal for caching and real-time access. Its support for advanced data 
structures like Hashes, Lists, Sets, and Sorted Sets makes it flexible for tasks such as counters, queues, or temporary session
 data. Additionally, Redis is simple to use, scalable, and supports optional persistence, giving both speed and flexibility 
 compared to traditional databases."
 
Key Features of Redis
In-Memory Storage
Stores data in RAM for extremely fast read/write operations.

Advanced Data Structures
Supports Strings, Hashes, Lists, Sets, Sorted Sets, Bitmaps, HyperLogLogs, Streams.
Makes Redis flexible for caching, queues, counters, leaderboards, etc.

Persistence Options
RDB (snapshotting) and AOF (append-only file) allow optional data persistence.

High Performance

Can handle millions of requests per second with sub-millisecond latency.

Scalability
Supports replication, clustering, and sharding for distributed applications.

Atomic Operations

Redis commands are atomic, ensuring safe operations on shared data.

Pub/Sub Messaging
Supports publish/subscribe, enabling real-time messaging and notifications.

Lightweight and Simple
Easy to integrate, minimal configuration, and less overhead than relational databases.

=============================================================================

Yes, Redis is a database, but with some special characteristics:

Why Redis is a Database
It stores and manages data, like other databases (MySQL, PostgreSQL, MongoDB).
You can read, write, and update data using commands.
It can optionally persist data to disk so it‚Äôs not lost after a restart.

What Makes Redis Special
It‚Äôs an in-memory database, meaning most of the data is stored in RAM for very fast access.
It‚Äôs often used as a cache, message broker, or temporary data store rather than for long-term relational storage.
Supports advanced data structures like Hashes, Lists, Sets, Sorted Sets, etc.

‚úÖ Simple Answer for Interview:
"Yes, Redis is a database. It‚Äôs an in-memory key-value database that provides extremely fast data access and can also be used
 for caching and temporary storage."


Why Redis is a Real Database, Not Just a Cache
Data Storage
Redis can store data permanently using RDB snapshots or AOF logs.
So even though it‚Äôs mostly in-memory, it can persist data like a normal database.

Supports Queries & Data Structures
Redis supports key-value operations and advanced data types like Hashes, Lists, Sets, Sorted Sets, Streams.
This makes it suitable for real application data, not only temporary cache.

Atomic Operations & Transactions
Redis supports atomic commands, transactions, and Lua scripting, which are features expected in a database.

Replication & High Availability
Redis supports replication, clustering, and sharding, enabling distributed database setups.

‚úÖ Sample Interview Answer:
"Redis is not just a cache; it‚Äôs a fully functional in-memory database. It can persist data, supports advanced data 
structures, atomic operations, and clustering, so it can act as both a fast cache and a reliable database depending on the 
application‚Äôs needs."
===========================================================

Why Redis is an In-Memory Database

Stores Data in RAM
Redis keeps its data in the main memory (RAM) instead of primarily on disk.
This allows extremely fast read and write operations, typically in sub-millisecond time.

Optimized for Speed
Disk-based databases (like MySQL, PostgreSQL) are slower because they need I/O operations to read/write data.
By keeping data in memory, Redis avoids disk I/O delays, making it ideal for caching, real-time analytics, and temporary storage.

Optional Persistence
Even though it‚Äôs in-memory, Redis can persist data to disk using RDB snapshots or AOF logs.
So it combines the speed of memory with the option for durability.

‚úÖ Simple Interview Answer:
"Redis is called an in-memory database because it stores data in RAM, which makes reads and writes extremely fast. It can
 also persist data to disk if needed, but its main advantage is speed for caching and real-time access."
 
==========================================================

Why Choose Elasticsearch

1Ô∏è‚É£ Full-Text Search Capability
Elasticsearch is optimized for searching large volumes of text quickly.
Supports partial matches, fuzzy search, relevance scoring, and complex queries that are difficult in traditional databases.

2Ô∏è‚É£ High Performance
Built on Lucene, it can index data efficiently and return search results in milliseconds, even for millions of records.

3Ô∏è‚É£ Scalability
Distributed by design: supports clustering, sharding, and replication.
Handles large datasets with high availability.

4Ô∏è‚É£ Analytics & Aggregations
Allows real-time analytics, like counts, averages, histograms, and trends.
Useful for dashboards, monitoring, and reporting.

5Ô∏è‚É£ RESTful API & Integration
Easy to integrate with applications using REST APIs or official clients (Java, Python, etc.).

6Ô∏è‚É£ Versatile Use Cases
Works as a search engine, analytics engine, and logging platform (e.g., with Kibana).

Sample Interview Answer
"We chose Elasticsearch because it provides fast, scalable, and powerful search capabilities. Its full-text search, analytics,
 and distributed nature make it ideal for querying large volumes of data in real time, which would be slow or complex with 
 traditional relational databases."
 
"Lucene is a high-performance Java library for full-text search. It creates indexes and quickly searches through large 
amounts of text. Elasticsearch uses Lucene under the hood to provide fast and scalable search capabilities."
 
======================================================================

Redundancy - 

Definition: Redundancy means having duplicate components, systems, or data so that if one fails, the other can take over.
Purpose: To prevent single points of failure.

Example:
Two identical database servers with the same data (replicas).
Multiple power supplies in a server.

High Availability (HA)
Definition: High Availability is the ability of a system to stay operational and accessible for a high percentage of time.
Purpose: To ensure the system is always available to users, even if some components fail.
How it‚Äôs achieved: Often using redundancy, failover mechanisms, and load balancing.

‚úÖ Key Difference:
Redundancy = duplication of components or data.
High Availability = system is continuously available.

Redundancy is one way to achieve high availability, but HA also involves monitoring, failover, and recovery strategies.

========================================================================

Explaining Redundancy in NEF with HSM (Corrected)

1Ô∏è‚É£ Redundancy Setup
In NEF, we achieved redundancy by running multiple NEF instances.
One instance is active, handling all requests.
Other instances are standby, ready to take over if the active instance fails.

2Ô∏è‚É£ Role of HSM
HSM manages the failover between NEF instances.
Requests from clients go to HSM‚Äôs virtual IP (VIP) instead of the NEF‚Äôs real IP.
HSM internally routes the request to the active NEF instance.

3Ô∏è‚É£ Failover Mechanism
If the active NEF goes down, HSM automatically directs requests to a standby NEF, which becomes active.
Ensures continuous operation without downtime.

4Ô∏è‚É£ Benefits
Achieves redundancy with multiple NEF instances.
Ensures high availability as HSM handles failover automatically.
Users experience no interruption, even during NEF failures.

‚úÖ Sample Interview Explanation (verbal version):
"In NEF, we achieved redundancy by running multiple NEF instances. One instance is active, handling requests, while others 
are standby. We integrated HSM, which uses a virtual IP (VIP). Requests go to the VIP, and HSM forwards them to the active NEF. 
If the active instance fails, HSM automatically redirects requests to a standby instance, ensuring high availability without 
downtime."

=====================================================================

1Ô∏è‚É£ Scalability in NEF

a. Horizontal Scaling (Multiple Instances)
We ran multiple NEF instances to handle increased load.
Used HSM with a virtual IP (VIP) to route requests to the active instance.
If load increases, standby instances can become active, distributing traffic efficiently.

b. Stateless Design
NEF was designed to be stateless, meaning each instance doesn‚Äôt depend on previous requests.
This makes horizontal scaling easier, as any instance can handle any request.

c. Caching with Redis
Frequently accessed data was stored in Redis (in-memory cache).
Reduced database load and improved response times for high request volumes.

d. Asynchronous Processing
Long-running operations (like heavy computations or external API calls) were offloaded to queues.
Prevented blocking and allowed the system to serve more requests concurrently.

2Ô∏è‚É£ Scalability in BR
a. Microservices / Modular Architecture
BR backend services were broken into small modules or microservices, each handling a specific task.
This allows independent scaling of services that have high demand.

b. Database Optimization
Used Elasticsearch for fast search and analytics instead of querying relational databases.
Indexing and caching reduced query times and improved throughput.

c. Load Balancing
Requests were routed via load balancers to multiple BR service instances.
Ensures no single instance is overloaded and improves availability and fault tolerance.

d. Asynchronous & Event-Driven Processing
Heavy operations were processed asynchronously using message queues or event-driven patterns.
Enables better concurrent request handling.

3Ô∏è‚É£ Key Points to Highlight in Interview
Horizontal scaling ‚Üí multiple instances to handle traffic.
Caching ‚Üí Redis to reduce DB load.
Stateless services ‚Üí easier scaling and failover.
Load balancing & HSM (NEF) ‚Üí route traffic efficiently.
Asynchronous processing ‚Üí handle more requests concurrently.
Using optimized databases ‚Üí Elasticsearch for fast search/analytics.

‚úÖ Sample Verbal Answer (Concise Version):
"In NEF and BR, we achieved scalability through multiple approaches. For NEF, we ran multiple stateless instances with HSM 
and a virtual IP for failover and horizontal scaling. Redis caching reduced database load, and long-running tasks were
 processed asynchronously. In BR, services were modular, using Elasticsearch for fast search and analytics, load balancers to
 distribute traffic, and event-driven processing for concurrency. These design choices allowed both systems to handle high 
 traffic efficiently and remain responsive."


Exactly! HSM contributes to both redundancy and scalability, but in slightly different ways:

1Ô∏è‚É£ Redundancy
HSM ensures high availability by managing multiple NEF instances.
Active instance handles requests, standby instances take over automatically if the active fails.
This prevents downtime ‚Üí system redundancy.

2Ô∏è‚É£ Scalability
HSM allows horizontal scaling of NEF instances.
As traffic increases, more NEF instances can be added behind the HSM VIP.
HSM automatically routes requests to handle increased load efficiently.

‚úÖ Simple way to explain in interview:
"HSM plays a dual role. For redundancy, it manages failover between NEF instances so the system remains available. 
For scalability, it allows multiple NEF instances to run in parallel, routing requests efficiently as traffic grows."


Perfect! Here‚Äôs a polished, interview-ready answer combining NEF redundancy, scalability, Redis caching, and BR backend 
scalability:

*"In NEF, we designed the backend for both redundancy and scalability. For redundancy, we ran multiple NEF instances‚Äîone active and 
others standby. HSM manages a virtual IP (VIP), routing all requests to the active instance, and automatically failing over to 
standby instances if the active instance goes down.

For scalability, HSM also allows horizontal scaling, so as traffic increases, more NEF instances can be added and requests 
are efficiently distributed. NEF was designed to be stateless, making horizontal scaling easy. We also used Redis as an 
in-memory cache to store frequently accessed data, which reduced database load and improved response times. Long-running 
operations were handled asynchronously to serve more requests concurrently.

In BR, we achieved scalability by using a modular service architecture, allowing high-demand services to scale independently.
 We used Elasticsearch for fast search and analytics instead of traditional databases, and load balancers to distribute 
 traffic across multiple service instances. Event-driven processing handled heavy operations asynchronously, ensuring high 
 throughput.

These design choices ensured both NEF and BR could handle high traffic efficiently, remain highly available, and deliver
 fast responses."*
 
===========================================================================

cut deployment time by 30% - autoinstall script

how you can explain in an interview how you cut deployment time by 30%:

1Ô∏è‚É£ Key Points to Highlight

Automation
Previously, deployments and rollbacks were likely manual, involving multiple commands to copy files, stop/start services, 
and backup configs.

With these scripts, the process is fully automated, reducing human intervention and errors.

Single-Step Execution
autoinstall.sh handles:
Checking if the container is running.
Verifying BR service status.
Creating backup of configuration, bin, and lib folders.
Extracting the new package zip and copying all necessary files to the container.

rollback.sh handles:
Restoring the previous version from backups automatically.
Both scripts run all tasks sequentially in a single script, saving time.
Error Checks & Safeguards
Scripts check if BR is running before deploying.
They verify directories exist and backups are present.
This prevents failures that could otherwise require manual troubleshooting.
Optimized File Handling
Only the required folders and files are copied, instead of manual step-by-step copying.
Old dist folders are deleted automatically, and backup management is included.
Container Awareness
Scripts work directly with Docker containers, so you don‚Äôt need to manually enter the container or stop/start processes.

2Ô∏è‚É£ How to Explain in Interview
"We cut deployment time by about 30% by automating the entire BR upgrade and rollback process. I wrote autoinstall.sh and 
rollback.sh scripts that handle all steps‚Äîfrom checking the container status, verifying BR service, creating backups,
 extracting the new package, and copying the necessary bin, lib, and config files‚Äîautomatically. Previously, these steps were
 manual, time-consuming, and error-prone. The scripts also include safeguards like checking if BR is running and ensuring 
 backups exist. Automation reduced human intervention, minimized errors, and made the deployment process much faster and 
 more reliable."


============================================================================

Redis Clustering & High Availability

1Ô∏è‚É£ High Availability via Replication
In Redis, high availability is achieved using replication:
Master node holds the primary data.
Replica nodes (slaves) maintain copies of the master.
If the master fails, a replica can be promoted to master automatically.

2Ô∏è‚É£ Clustering for Scalability & Availability
Redis cluster splits data across multiple nodes (sharding) for horizontal scaling.
Each shard can have replicas for redundancy, ensuring both high availability and fault tolerance.

‚úÖ Sample Interview Explanation:
"In NEF, we integrated Redis as a caching layer for faster data retrieval. For high availability, we used Redis replication,
 where replica nodes maintain copies of the master. In case the master fails, a replica takes over. Additionally, Redis 
 clustering allowed us to scale horizontally, distributing data across multiple nodes while maintaining replicas for 
 redundancy."
 
============================================================================

You can explain ‚Äúreal-time data indexing, fast search, optimized query performance across large datasets‚Äù in simple, 
interview-friendly terms like this:

How to Explain It
1Ô∏è‚É£ Real-Time Data Indexing
Whenever new backup data was created, it was immediately stored in Elasticsearch.
This means users could search or access the latest data instantly without waiting.

2Ô∏è‚É£ Fast Search
Elasticsearch is designed to quickly find data even when there‚Äôs a lot of it.
So searching backups by name, date, or type was almost instantaneous.

3Ô∏è‚É£ Handling Large Datasets Efficiently
Elasticsearch can store and manage millions of records efficiently.
Even with huge datasets, searches remained fast and responsive, unlike traditional databases.

Sample Interview Answer
"In BR, I integrated Elasticsearch to make backup data immediately searchable. Each time a backup was created, it was indexed 
in real-time, allowing users to search instantly. Elasticsearch ensured that even with large volumes of data, search and 
retrieval were fast and responsive, improving overall system performance."

==================================================================================

In general terms, indexing means creating a system or structure to locate and access information quickly.

Simple Explanation
Think of a book index: it lists topics and the pages they appear on, so you can find information without reading the whole 
book.

Similarly, in computers or databases, indexing organizes data so you can search, retrieve, or sort it efficiently.

Key Points
Makes searching faster.
Reduces the need to scan all data.
Used in databases, search engines, and even libraries.

‚úÖ Simple Interview Answer:
"Indexing is a way of organizing data to make searching and retrieving information faster and more efficient, like an index 
in a book."


========================================================================================

‚úÖ Interview Explanation Example:
"In my project, we use an embedded Jetty server so our application is self-contained and doesn‚Äôt require a separate servlet container. We use Ant to build the project, producing an executable JAR. This JAR is then containerized using Docker, which makes deployment consistent and isolated. On the server, we upload this JAR inside a container and run it. When the container starts, Jetty starts inside the JAR, serving the application."

No ‚Äî you cannot use HttpServlet without a server (Tomcat, Jetty, etc.), because servlets are designed to run inside a Servlet Container that implements the Servlet API.

Here‚Äôs why:
1. Why HttpServlet Needs a Server
HttpServlet is part of the Jakarta Servlet API (javax.servlet / jakarta.servlet).
It defines methods like doGet() and doPost() for handling HTTP requests.
But it does not implement HTTP networking itself ‚Äî it needs a server (Tomcat, Jetty, Undertow, etc.) to:
Listen on a port (HTTP requests).
Receive requests and map URLs.
Create HttpServletRequest and HttpServletResponse objects.
Call your servlet methods (doGet, doPost, etc.).

Without a servlet container, HttpServlet has no way to run or receive HTTP requests.

Analogy:
Think of HttpServlet like a driver‚Äôs manual. The manual explains what the driver does, but it doesn‚Äôt drive the car. You still need the car (server) to actually move.

2. How Embedded Jetty Helps

When you use embedded Jetty, you include the server inside your application. That means you don‚Äôt need a separate external Tomcat or Jetty installation ‚Äî but you still have a server running internally.

Example:

Server server = new Server(8080);
ServletContextHandler context = new ServletContextHandler();
context.addServlet(MyServlet.class, "/hello");
server.setHandler(context);
server.start();
server.join();

Here Jetty is your server, and it handles the HTTP part so your servlet can run.

3. Alternatives if You Don‚Äôt Want a Server
If you really don‚Äôt want Jetty/Tomcat, then you cannot use HttpServlet. You would need to use something else that handles HTTP directly:
HTTP framework like:
Spring Boot (which internally embeds Tomcat/Jetty/Undertow)
NanoHTTPD (very lightweight HTTP server in Java)
Or manually write an HTTP server using java.net.ServerSocket ‚Äî but then you must manually parse HTTP requests, which is complex.

Example with plain Java:

ServerSocket serverSocket = new ServerSocket(8080);
while (true) {
    Socket client = serverSocket.accept();
    // Manually read HTTP request and send response
}


This works but is low-level and not practical for production.

‚úÖ Key takeaway:
You cannot run HttpServlet without a server. If you want a lightweight approach without an external server, embedded Jetty or embedded Tomcat is the way to go ‚Äî that‚Äôs why your project uses embedded Jetty.
===================================================================================
